---
phase: 01-seo-foundation
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - apps/web/src/app/sitemap.ts
  - apps/web/src/app/robots.ts
autonomous: true

must_haves:
  truths:
    - "Sitemap at /sitemap.xml lists homepage, /hogskoleprovet, and all 26 test pages"
    - "Robots.txt at /robots.txt allows crawling and references sitemap"
    - "Sitemap uses same data source as generateStaticParams (tests array)"
  artifacts:
    - path: "apps/web/src/app/sitemap.ts"
      provides: "Dynamic sitemap generation"
      exports: ["default"]
      contains: "MetadataRoute.Sitemap"
    - path: "apps/web/src/app/robots.ts"
      provides: "Robots.txt generation"
      exports: ["default"]
      contains: "MetadataRoute.Robots"
  key_links:
    - from: "apps/web/src/app/sitemap.ts"
      to: "@/data/tests"
      via: "import tests array"
      pattern: "import.*tests.*from.*@/data/tests"
    - from: "apps/web/src/app/sitemap.ts"
      to: "test.slug"
      via: "map over tests for URLs"
      pattern: "tests\\.map"
---

<objective>
Create sitemap.xml and robots.txt using Next.js 15 file conventions

Purpose: Enable search engines to discover and efficiently crawl all test pages. Sitemap tells Google exactly which URLs exist; robots.txt confirms crawling is allowed and points to sitemap.

Output: Two new files that generate /sitemap.xml and /robots.txt at runtime
</objective>

<execution_context>
@/Users/williamlarsten/.claude/get-shit-done/workflows/execute-plan.md
@/Users/williamlarsten/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/01-seo-foundation/01-RESEARCH.md

# Existing files to understand data structure
@apps/web/src/data/tests.ts
@apps/web/src/app/hogskoleprovet/[slug]/page.tsx
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create sitemap.ts</name>
  <files>apps/web/src/app/sitemap.ts</files>
  <action>
Create `apps/web/src/app/sitemap.ts` that generates a sitemap with:

1. Import `tests` from `@/data/tests` (same source as generateStaticParams)
2. Define BASE_URL as `https://maxa.se` (production domain)
3. Export default function returning `MetadataRoute.Sitemap`
4. Include static routes:
   - Homepage (`/`) with priority 1, changeFrequency 'monthly'
   - List page (`/hogskoleprovet`) with priority 0.9, changeFrequency 'weekly'
5. Map over `tests` array to generate test page entries:
   - URL: `${BASE_URL}/hogskoleprovet/${test.slug}`
   - lastModified: `new Date(test.date)` (use actual test date, not build date)
   - changeFrequency: 'monthly'
   - priority: 0.8

Pattern from research:
```typescript
import type { MetadataRoute } from 'next'
import { tests } from '@/data/tests'

const BASE_URL = 'https://maxa.se'

export default function sitemap(): MetadataRoute.Sitemap {
  // ... implementation
}
```

Important: Use the exact same `tests` import that generateStaticParams uses to prevent sitemap/page drift.
  </action>
  <verify>
Run `bun dev:web` and visit `http://localhost:3000/sitemap.xml`
- Should return valid XML
- Should contain homepage URL
- Should contain /hogskoleprovet URL
- Should contain all 26 test page URLs (e.g., /hogskoleprovet/hosten-2025)
- Each test URL should have lastModified matching test.date
  </verify>
  <done>
/sitemap.xml returns valid XML listing 28 URLs (homepage + list page + 26 tests) with correct priorities and lastModified dates
  </done>
</task>

<task type="auto">
  <name>Task 2: Create robots.ts</name>
  <files>apps/web/src/app/robots.ts</files>
  <action>
Create `apps/web/src/app/robots.ts` that generates robots.txt with:

1. Export default function returning `MetadataRoute.Robots`
2. Rules object with:
   - userAgent: '*' (all crawlers)
   - allow: '/' (allow everything by default)
   - disallow: ['/api/', '/_next/'] (block internal routes)
3. sitemap: 'https://maxa.se/sitemap.xml' (reference the sitemap)

Pattern from research:
```typescript
import type { MetadataRoute } from 'next'

export default function robots(): MetadataRoute.Robots {
  return {
    rules: {
      userAgent: '*',
      allow: '/',
      disallow: ['/api/', '/_next/'],
    },
    sitemap: 'https://maxa.se/sitemap.xml',
  }
}
```
  </action>
  <verify>
Run `bun dev:web` and visit `http://localhost:3000/robots.txt`
- Should return text/plain content
- Should contain "User-agent: *"
- Should contain "Allow: /"
- Should contain "Disallow: /api/"
- Should contain "Disallow: /_next/"
- Should contain "Sitemap: https://maxa.se/sitemap.xml"
  </verify>
  <done>
/robots.txt returns valid robots.txt file allowing crawling of public pages, blocking internal routes, and referencing the sitemap
  </done>
</task>

</tasks>

<verification>
1. Start dev server: `bun dev:web`
2. Test sitemap: `curl http://localhost:3000/sitemap.xml`
   - Valid XML structure
   - Contains 28 URLs total
3. Test robots: `curl http://localhost:3000/robots.txt`
   - Contains User-agent, Allow, Disallow, Sitemap directives
4. Cross-reference: Sitemap URLs should match what generateStaticParams produces
   - Count tests in tests.ts array (should be 26)
   - Count test URLs in sitemap (should also be 26)
</verification>

<success_criteria>
- /sitemap.xml accessible and returns valid XML with all test pages
- /robots.txt accessible and returns valid robots.txt with sitemap reference
- No TypeScript errors
- Both files use Next.js MetadataRoute types correctly
</success_criteria>

<output>
After completion, create `.planning/phases/01-seo-foundation/01-01-SUMMARY.md`
</output>
